{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7eHGTSKv8TW"
      },
      "source": [
        "# Project Notebook: Optimizing DataFrames and Processing in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ7UJlKnwJk3"
      },
      "source": [
        "## 1. Introduction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUyK3GcBv9mC"
      },
      "source": [
        "In this project, we'll practice working with chunked dataframes and optimizing a dataframe's memory usage. We'll be working with financial lending data from Lending Club, a marketplace for personal loans that matches borrowers with investors. You can read more about the marketplace on its website.\n",
        "\n",
        "The Lending Club's website lists approved loans. Qualified investors can view the borrower's credit score, the purpose of the loan, and other details in the loan applications. Once a lender is ready to back a loan, it selects the amount of money it wants to fund. When the loan amount the borrower requested is fully funded, the borrower receives the money, minus the origination fee that Lending Club charges.\n",
        "\n",
        "We'll be working with a dataset of loans approved from 2007-2011 (https://bit.ly/3H2XVgC). We've already removed the desc column for you to make our system run more quickly.\n",
        "\n",
        "If we read in the entire data set, it will consume about 67 megabytes of memory. Let's imagine that we only have 10 megabytes of memory available throughout this project, so you can practice the concepts you learned in the last two lessons.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Read in the first five lines from `loans_2007.csv` (https://bit.ly/3H2XVgC) and look for any data quality issues.\n",
        "\n",
        "2. Read in the first 1000 rows from the data set, and calculate the total memory usage for these rows. Increase or decrease the number of rows to converge on a memory usage under five megabytes (to stay on the conservative side)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc9NLSZ5vXPM"
      },
      "source": [
        "# Importing pandas\n",
        "# import pandas as pd\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read in the first five lines of the loans_2007.csv file\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "nrows=5\n",
        "\n",
        "\n",
        "# Check for data quality issues\n",
        "df.info()\n",
        "df.describe()\n",
        "df.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in the first 1000 rows of the loans_2007.csv file\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "nrows=1000\n",
        "\n",
        "# Calculate the memory usage of the dataframe\n",
        "df_memory_usage = df.memory_usage().sum() / 1024**2\n",
        "print(f\"Total memory usage: {df_memory_usage:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "aKjckgtnS0HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StemrBe2wkMj"
      },
      "source": [
        "## 2. Exploring the Data in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm4p982Dwor5"
      },
      "source": [
        "Let's familiarize ourselves with the columns to see which ones we can optimize. In the first lesson, we explored column types by reading in the full dataframe. In this project, let's try to understand the column types better while using dataframe chunks.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "For each chunk:\n",
        "* How many columns have a numeric type? \n",
        "* How many have a string type?\n",
        "* How many unique values are there in each string column? How many of the string columns contain values that are less than 50% unique?\n",
        "* Which float columns have no missing values and could be candidates for conversion to the integer type?\n",
        "* Calculate the total memory usage across all of the chunks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the columns with a numeric data type\n",
        "numeric_columns = df.select_dtypes(include=[\"int\", \"float\"])\n",
        "\n",
        "# Count the number of columns\n",
        "num_numeric_columns = numeric_columns.size\n",
        "\n",
        "# Print the number of numeric columns\n",
        "print(num_numeric_columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "HSvtMUGYUcia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the columns with a string data type\n",
        "string_columns = df.select_dtypes(include=[\"object\"])\n",
        "\n",
        "# Count the number of columns\n",
        "num_string_columns = string_columns.size\n",
        "\n",
        "# Print the number of string columns\n",
        "print(num_string_columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "eCsk028_U4J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the columns with a string data type\n",
        "string_columns = df.select_dtypes(include=[\"object\"])\n",
        "\n",
        "# Iterate through the string columns\n",
        "for column in string_columns:\n",
        "    # Get the number of unique values\n",
        "    num_unique = df[column].nunique()\n",
        "    \n",
        "    # Get the total number of values\n",
        "    num_total = df[column].size\n",
        "    \n",
        "    # Calculate the percentage of unique values\n",
        "    pct_unique = num_unique / num_total\n",
        "    \n",
        "    # Print the column name and percentage of unique values\n",
        "    print(f\"Column {column}: {pct_unique:.2%} unique\")\n",
        "\n"
      ],
      "metadata": {
        "id": "K1BCotKCU-zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize a list to store the names of the float columns with no missing values\n",
        "members = []\n",
        "\n",
        "# Iterate over the dataframe chunks\n",
        "for chunk in pd.read_csv(\"https://bit.ly/3H2XVgC\", chunksize=1000):\n",
        "  # Select only the float columns\n",
        "  float_columns = chunk.select_dtypes(include='float')\n",
        "\n",
        "  # Check if the float columns have any missing values\n",
        "  no_missing_values = float_columns.isnull().sum().eq(0)\n",
        "\n",
        "  # Get the names of the float columns with no missing values\n",
        "  chunk_members = no_missing_values[no_missing_values.all()].index\n",
        "\n",
        "  # Add the float columns with no missing values to the list\n",
        "  members.extend(chunk_members)\n",
        "\n",
        "print(members)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2SgYtvPoV_BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize a variable to store the total memory usage\n",
        "total_memory_usage = 0\n",
        "\n",
        "# Iterate over the dataframe chunks\n",
        "for chunk in pd.read_csv(\"https://bit.ly/3H2XVgC\" , chunksize=1000):\n",
        "  # Calculate the memory usage of each column\n",
        "  column_memory_usage = chunk.memory_usage()\n",
        "\n",
        "  # Add the total memory usage of the chunk to the total\n",
        "  total_memory_usage += column_memory_usage.sum()\n",
        "\n",
        "print(total_memory_usage)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx1klmMMfbCa",
        "outputId": "956b1bd2-4a84-41e5-b461-41560d7e00a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17701480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BtAVP8fw3OH"
      },
      "source": [
        "## 3. Optimizing String Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkWArHAhw_bw"
      },
      "source": [
        "We can achieve the greatest memory improvements by converting the string columns to a numeric type. Let's convert all of the columns where the values are less than 50% unique to the category type, and the columns that contain numeric values to the `float` type.\n",
        "\n",
        "While working with dataframe chunks:\n",
        "* Determine which string columns you can convert to a numeric type if you clean them. For example, the `int_rate` column is only a string because of the % sign at the end.\n",
        "* Determine which columns have a few unique values and convert them to the category type. For example, you may want to convert the grade and `sub_grade` columns.\n",
        "Based on your conclusions, perform the necessary type changes across all chunks. * Calculate the total memory footprint, and compare it with the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH0tcQlpxG9s"
      },
      "source": [
        "# Your code goes here\n",
        "import pandas as pd\n",
        "\n",
        "# Read in the dataframe chunk\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "\n",
        "# Remove the % sign from the int_rate column and convert it to a float\n",
        "df[\"int_rate\"] = df[\"int_rate\"].str.rstrip(\"%\").astype(float)\n",
        "\n",
        "# Convert the grade and sub_grade columns to the category type\n",
        "df[\"grade\"] = df[\"grade\"].astype(\"category\")\n",
        "df[\"sub_grade\"] = df[\"sub_grade\"].astype(\"category\")\n",
        "\n",
        "# Calculate the total memory footprint of the dataframe\n",
        "total_memory = df.memory_usage(deep=True).sum()\n",
        "print(\"Total memory usage:\", total_memory)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22oYzgXnxIcV"
      },
      "source": [
        "## 4. Optimizing Numeric Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5C20YrxPID"
      },
      "source": [
        "It looks like we were able to realize some powerful memory savings by converting to the category type and converting string columns to numeric ones.\n",
        "\n",
        "Now let's optimize the numeric columns using the `pandas.to_numeric()` function.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "While working with dataframe chunks:\n",
        "* Identify float columns that contain missing values, and that we can convert to a more space efficient subtype.\n",
        "* Identify float columns that don't contain any missing values, and that we can convert to the integer type because they represent whole numbers.\n",
        "* Based on your conclusions, perform the necessary type changes across all chunks.\n",
        "* Calculate the total memory footprint and compare it with the previous one.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S9KR57LxQ9d"
      },
      "source": [
        "# Your code goes here\n",
        "# Select float columns\n",
        "float_cols = df.select_dtypes(include='float').columns\n",
        "\n",
        "# Select float columns with missing values\n",
        "float_cols_with_missing = df[float_cols].isnull().sum()[df[float_cols].isnull().sum() > 0]\n",
        "\n",
        "# Print float columns with missing values\n",
        "print(float_cols_with_missing)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select float columns\n",
        "float_cols = df.select_dtypes(include='float').columns\n",
        "\n",
        "# Select float columns with no missing values\n",
        "float_cols_no_missing = df[float_cols].isnull().sum()[df[float_cols].isnull().sum() == 0]\n",
        "\n",
        "# Print float columns with no missing values\n",
        "print(float_cols_no_missing)\n",
        "\n",
        "# Convert float columns to integers\n",
        "\n",
        "\n",
        "\n",
        "# Iterate over the chunks\n",
        "for chunk in pd.read_csv(\"https://bit.ly/3H2XVgC\" , chunksize=chunksize):\n",
        "    # Select float columns\n",
        "    float_cols = chunk.select_dtypes(include='float').columns\n",
        "    \n",
        "    # Convert float columns with missing values to float32\n",
        "    chunk[float_cols] = chunk[float_cols].astype('float32')\n",
        "    \n",
        "    # Select float columns with no missing values\n",
        "    float_cols_no_missing = chunk[float_cols].isnull().sum()[chunk[float_cols].isnull().sum() == 0]\n",
        "    \n",
        "    # Convert float columns with no missing values to integers\n",
        "    chunk[float_cols_no_missing.index] = pd.to_numeric(chunk[float_cols_no_missing.index], downcast='integer')\n",
        "    \n",
        "    # Append chunk to list\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# Concatenate the chunks and save to a new DataFrame\n",
        "df_optimized = pd.concat(chunks)\n",
        "\n"
      ],
      "metadata": {
        "id": "i_v4tTC7ki6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Calculate the total memory usage of the DataFrame\n",
        "total_memory_usage = df.memory_usage(deep=True).sum()\n",
        "\n",
        "# Calculate the total memory usage of the optimized DataFrame\n",
        "optimized_memory_usage = df_optimized.memory_usage(deep=True).sum()\n",
        "\n",
        "# Print the memory usage of the DataFrame and the optimized DataFrame\n",
        "print(\"Total memory usage:\", total_memory_usage)\n",
        "print(\"Optimized memory usage:\", optimized_memory_usage)\n",
        "\n",
        "# Calculate the difference in memory usage\n",
        "memory_saving = total_memory_usage - optimized_memory_usage\n",
        "\n",
        "# Print the difference in memory usage\n",
        "print(\"Memory saving:\", memory_saving)\n"
      ],
      "metadata": {
        "id": "TqwdN_hJmgZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMJVaj_kxj42"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0RL3BzexlkW"
      },
      "source": [
        "We've practiced optimizing a dataframe's memory footprint and working with dataframe chunks. Here's an idea for some next steps:\n",
        "\n",
        "Create a function that automates as much of the work you just did as possible, so that you could use it on other Lending Club data sets. This function should:\n",
        "\n",
        "* Determine the optimal chunk size based on the memory constraints you provide.\n",
        "\n",
        "* Determine which string columns can be converted to numeric ones by removing the `%` character.\n",
        "\n",
        "* Determine which numeric columns can be converted to more space efficient representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_optimal_chunk_size(df, memory_limit):\n",
        "  # Calculate the size of the dataframe\n",
        "  df_size = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
        "\n",
        "  # Calculate the optimal chunk size\n",
        "  chunk_size = int(memory_limit / df_size * len(df))\n",
        "\n",
        "  return chunk_size\n",
        "\n",
        "  memory_limit = 1024 # Set the memory limit to 1GB\n",
        "\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\") # Read in the dataframe\n",
        "\n",
        "chunk_size = get_optimal_chunk_size(df, memory_limit)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AQqw8c0rvXRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_numeric_columns(df):\n",
        "  numeric_columns = []\n",
        "  for col in df.select_dtypes(include=['object']).columns:\n",
        "    try:\n",
        "      df[col] = pd.to_numeric(df[col].str.strip().str.replace('%', ''))\n",
        "      numeric_columns.append(col)\n",
        "    except ValueError:\n",
        "      pass\n",
        "  return numeric_columns\n",
        "\n",
        "  df = pd.read_csv(\"https://bit.ly/3H2XVgC\") # Read in the dataframe\n",
        "numeric_columns = get_numeric_columns(df)\n"
      ],
      "metadata": {
        "id": "BCwZpFi5wU_s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine which numeric columns can be converted to more space efficient representations.\n",
        "\n",
        "def get_space_efficient_columns(df):\n",
        "  space_efficient_columns = []\n",
        "  for col in df.select_dtypes(include=['float']).columns:\n",
        "    df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    space_efficient_columns.append(col)\n",
        "  for col in df.select_dtypes(include=['int']).columns:\n",
        "    df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "    space_efficient_columns.append(col)\n",
        "  return space_efficient_columns\n",
        "\n",
        "df = pd.read_csv(\"https://bit.ly/3H2XVgC\") # Read in the dataframe\n",
        "space_efficient_columns = get_space_efficient_columns(df)\n"
      ],
      "metadata": {
        "id": "giIK0zQxwsJz"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}